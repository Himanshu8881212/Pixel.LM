"""
Regression tests and bug-catching tests for DeepSeek VL2.

These tests target specific failure points identified through code review:
1. Projector dimension mismatches
2. Image embedding insertion bugs
3. MoE routing issues
4. Attention mask handling
5. KV cache corruption
6. Config loading edge cases
7. Weight tying issues
8. Gradient flow interruptions
"""

import pytest
import torch
import torch.nn as nn
import tempfile
import math
from pathlib import Path

import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.config import (
    VisionConfig,
    ProjectorConfig,
    LanguageConfig,
    ModelConfig,
    MLAConfig,
    MoEConfig,
    DeepSeekVL2TrainingConfig,
    load_config_with_overrides,
    save_config,
    load_config,
)
from src.models.vision_encoder import SigLIPVisionEncoder, PatchEmbed
from src.models.projector import MlpProjector
from src.models.mla import MultiHeadLatentAttention, RMSNorm, RotaryEmbedding
from src.models.moe import MoELayer, MoEGate
from src.models.language_model import DecoderLayer, DeepSeekMoELanguageModel
from src.models.deepseek_vl2 import DeepSeekVL2ForPretraining


@pytest.fixture
def device():
    return "cuda" if torch.cuda.is_available() else "cpu"


# =============================================================================
# BUG #1: Projector Dimension Mismatch with Non-Square Patch Counts
# =============================================================================

class TestProjectorBugs:
    """Test projector-related bugs."""
    
    def test_projector_non_square_patch_count(self):
        """
        BUG: Projector assumes h = w = sqrt(hw), but this fails for non-square counts.
        Line 70 in projector.py: h = w = int(hw ** 0.5)
        """
        config = ProjectorConfig(
            type="downsample_mlp_gelu",
            input_dim=256,
            output_dim=512,
            downsample_ratio=2,
        )
        projector = MlpProjector(config)
        
        # Non-square patches (e.g., from different tiling)
        # 200 patches - sqrt(200) = 14.14... which truncates to 14
        # 14 * 14 = 196 != 200, causing data loss!
        x = torch.randn(2, 200, 256)
        
        # This should raise or handle gracefully
        try:
            out = projector(x)
            # If it runs, check no data was silently dropped
            # With 200 patches, sqrt gives ~14.14, int gives 14
            # Reshape to 14x14 = 196 drops 4 patches!
            # After downsample by 2: 7x7 = 49
            assert out.shape == (2, 49, 512)
        except Exception:
            # Expected - should handle gracefully
            pass
    
    def test_projector_downsample_ratio_not_dividing_evenly(self):
        """
        BUG: If h % downsample_ratio != 0, padding is added but may not be correct.
        """
        config = ProjectorConfig(
            type="downsample_mlp_gelu",
            input_dim=256,
            output_dim=512,
            downsample_ratio=3,  # Doesn't divide 14 evenly
        )
        projector = MlpProjector(config)
        
        # 196 patches = 14x14, but 14 % 3 = 2, needs padding
        x = torch.randn(2, 196, 256)
        out = projector(x)
        
        # After padding to 15x15 = 225, then downsample by 3: 5x5 = 25
        assert out.shape[1] > 0  # Should produce output
    
    def test_projector_input_dim_mismatch(self):
        """
        BUG: If vision hidden_size != projector input_dim, forward will crash.
        This is partially fixed in deepseek_vl2.py line 38, but only at init.
        """
        vision_hidden = 1024
        projector_input = 512  # MISMATCH!
        
        vision_config = VisionConfig(hidden_size=vision_hidden)
        projector_config = ProjectorConfig(
            input_dim=projector_input,  # Wrong!
            output_dim=2048,
        )
        
        # The model init should fix this
        model_config = ModelConfig(
            vision=vision_config,
            projector=projector_config,
            language=LanguageConfig(
                hidden_size=2048, num_layers=1,
                mla=MLAConfig(q_lora_rank=1024, kv_lora_rank=512, qk_rope_head_dim=32, qk_nope_head_dim=32, v_head_dim=64),
                moe=MoEConfig(enabled=False)
            ),
        )
        
        # Model init should auto-correct projector input_dim
        model = DeepSeekVL2ForPretraining(model_config)
        assert model.config.projector.input_dim == vision_hidden


# =============================================================================
# BUG #2: Image Embedding Insertion Logic
# =============================================================================

class TestImageInsertionBugs:
    """Test image embedding insertion bugs."""
    
    def test_image_position_beyond_sequence(self, device):
        """
        BUG: If image_position + num_image_tokens > seq_len, silently clipped.
        Lines 163-169 in deepseek_vl2.py handle this but may drop image tokens.
        """
        config = ModelConfig(
            vision=VisionConfig(image_size=56, hidden_size=64, num_layers=1, num_heads=2, patch_size=8),
            projector=ProjectorConfig(input_dim=64, output_dim=128, type="linear"),
            language=LanguageConfig(
                hidden_size=128, num_layers=1, vocab_size=256,
                mla=MLAConfig(q_lora_rank=64, kv_lora_rank=32, qk_rope_head_dim=8, qk_nope_head_dim=8, v_head_dim=16),
                moe=MoEConfig(enabled=False)
            ),
        )
        model = DeepSeekVL2ForPretraining(config).to(device)
        
        seq_len = 32
        input_ids = torch.randint(0, 256, (1, seq_len), device=device)
        images = torch.randn(1, 3, 56, 56, device=device)
        # Position near end - not enough room for all image tokens
        image_positions = torch.tensor([[seq_len - 5]], device=device)
        
        # Should not crash
        outputs = model(input_ids=input_ids, images=images, image_positions=image_positions)
        assert outputs["logits"].shape == (1, seq_len, 256)
    
    def test_multiple_images_overlapping_positions(self, device):
        """
        BUG: If multiple images have overlapping positions, later images overwrite earlier.
        """
        config = ModelConfig(
            vision=VisionConfig(image_size=56, hidden_size=64, num_layers=1, num_heads=2, patch_size=8),
            projector=ProjectorConfig(input_dim=64, output_dim=128, type="linear"),
            language=LanguageConfig(
                hidden_size=128, num_layers=1, vocab_size=256,
                mla=MLAConfig(q_lora_rank=64, kv_lora_rank=32, qk_rope_head_dim=8, qk_nope_head_dim=8, v_head_dim=16),
                moe=MoEConfig(enabled=False)
            ),
        )
        model = DeepSeekVL2ForPretraining(config).to(device)
        
        # Two images, overlapping positions
        input_ids = torch.randint(0, 256, (1, 100), device=device)
        images = torch.randn(1, 2, 3, 56, 56, device=device)  # 2 images
        image_positions = torch.tensor([[10, 15]], device=device)  # Overlapping!
        
        # Should not crash but behavior is undefined
        outputs = model(input_ids=input_ids, images=images, image_positions=image_positions)
        assert outputs["logits"] is not None
    
    def test_image_embed_batch_mismatch(self, device):
        """
        BUG: Line 166-168 uses img_idx to index image_embeds which is indexed by batch.
        """
        config = ModelConfig(
            vision=VisionConfig(image_size=56, hidden_size=64, num_layers=1, num_heads=2, patch_size=8),
            projector=ProjectorConfig(input_dim=64, output_dim=128, type="linear"),
            language=LanguageConfig(
                hidden_size=128, num_layers=1, vocab_size=256,
                mla=MLAConfig(q_lora_rank=64, kv_lora_rank=32, qk_rope_head_dim=8, qk_nope_head_dim=8, v_head_dim=16),
                moe=MoEConfig(enabled=False)
            ),
        )
        model = DeepSeekVL2ForPretraining(config).to(device)
        
        # Single image, but accessing with batch_idx and img_idx
        input_ids = torch.randint(0, 256, (2, 64), device=device)
        images = torch.randn(2, 3, 56, 56, device=device)  # One per batch
        image_positions = torch.tensor([[16], [20]], device=device)
        
        outputs = model(input_ids=input_ids, images=images, image_positions=image_positions)
        assert outputs["logits"].shape[0] == 2


# =============================================================================
# BUG #3: MoE Routing Issues
# =============================================================================

class TestMoEBugs:
    """Test MoE-related bugs."""
    
    def test_moe_expert_load_imbalance(self, device):
        """
        Check if all experts are being used (no dead experts).
        """
        config = LanguageConfig(
            hidden_size=256,
            moe=MoEConfig(
                enabled=True,
                num_experts=8,
                num_experts_per_token=2,
            )
        )
        moe = MoELayer(config).to(device)
        
        # Run multiple batches
        expert_usage = torch.zeros(8)
        for _ in range(100):
            x = torch.randn(8, 32, 256, device=device)
            x_flat = x.view(-1, 256)
            _, indices, _ = moe.gate(x_flat)
            for idx in indices.flatten():
                expert_usage[idx.item()] += 1
        
        # All experts should have some usage
        # (In practice, some imbalance is normal, but none should be 0)
        for i, usage in enumerate(expert_usage):
            assert usage > 0, f"Expert {i} is dead (never selected)"
    
    def test_moe_weight_sum_numerical_stability(self, device):
        """
        BUG: Normalizing weights by their sum can cause NaN if sum is 0.
        Line 42 in moe.py: topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)
        """
        gate = MoEGate(hidden_size=256, num_experts=8, num_experts_per_token=2).to(device)
        
        # Try with extreme values that could cause numerical issues
        x = torch.randn(64, 256, device=device) * 1e-10  # Very small
        weights, indices, logits = gate(x)
        
        # Weights should still be valid
        assert torch.isfinite(weights).all()
        assert torch.allclose(weights.sum(dim=-1), torch.ones(64, device=device), atol=1e-5)
    
    def test_moe_routing_gradient_flow(self, device):
        """
        Test gradients flow through MoE routing.
        """
        config = LanguageConfig(
            hidden_size=256,
            moe=MoEConfig(
                enabled=True,
                num_experts=4,
                num_experts_per_token=2,
            )
        )
        moe = MoELayer(config).to(device)
        
        x = torch.randn(2, 16, 256, device=device, requires_grad=True)
        out = moe(x)
        loss = out.sum()
        loss.backward()
        
        # Input should have gradients
        assert x.grad is not None
        assert torch.isfinite(x.grad).all()
        
        # Gate should have gradients
        assert moe.gate.gate.weight.grad is not None


# =============================================================================
# BUG #4: Attention and KV Cache Issues
# =============================================================================

class TestAttentionBugs:
    """Test attention-related bugs."""
    
    def test_kv_cache_length_mismatch(self, device):
        """
        BUG: If past_key_value has different length than expected, can crash.
        """
        config = LanguageConfig(
            hidden_size=256, num_heads=4,
            mla=MLAConfig(q_lora_rank=128, kv_lora_rank=64, qk_rope_head_dim=16, qk_nope_head_dim=16, v_head_dim=32),
        )
        mla = MultiHeadLatentAttention(config).to(device)
        
        # First pass
        x1 = torch.randn(2, 16, 256, device=device)
        out1, past_kv = mla(x1, use_cache=True)
        
        # Second pass with cache
        x2 = torch.randn(2, 8, 256, device=device)
        out2, _ = mla(x2, past_key_value=past_kv, use_cache=True)
        
        # Should handle the length difference
        assert out2.shape == (2, 8, 256)
    
    def test_position_ids_with_kv_cache(self, device):
        """
        BUG: Position IDs must account for cached sequence length.
        Line 176 in mla.py creates position_ids from 0, ignoring cache.
        """
        config = LanguageConfig(
            hidden_size=256, num_heads=4,
            mla=MLAConfig(q_lora_rank=128, kv_lora_rank=64, qk_rope_head_dim=16, qk_nope_head_dim=16, v_head_dim=32),
            max_position_embeddings=4096,
        )
        mla = MultiHeadLatentAttention(config).to(device)
        
        # First pass with explicit position_ids
        x1 = torch.randn(2, 16, 256, device=device)
        pos1 = torch.arange(16, device=device).unsqueeze(0)
        out1, past_kv = mla(x1, position_ids=pos1, use_cache=True)
        
        # Second pass - should continue from position 16
        x2 = torch.randn(2, 4, 256, device=device)
        pos2 = torch.arange(16, 20, device=device).unsqueeze(0)
        out2, _ = mla(x2, position_ids=pos2, past_key_value=past_kv, use_cache=True)
        
        assert out2.shape == (2, 4, 256)
    
    def test_rope_dimension_mismatch(self):
        """
        BUG: RoPE dim must match qk_rope_head_dim exactly.
        """
        rope_dim = 64
        mla_rope_dim = 64
        
        rope = RotaryEmbedding(dim=rope_dim, max_position_embeddings=4096)
        
        assert rope.dim == mla_rope_dim
        assert rope.inv_freq.shape[0] == rope_dim // 2


# =============================================================================
# BUG #5: Weight Tying Issues
# =============================================================================

class TestWeightTyingBugs:
    """Test weight tying bugs."""
    
    def test_weight_tying_after_load(self, device):
        """
        BUG: Weight tying may break after loading checkpoint.
        """
        config = ModelConfig(
            vision=VisionConfig(image_size=56, hidden_size=64, num_layers=1, num_heads=2, patch_size=8),
            projector=ProjectorConfig(input_dim=64, output_dim=128, type="linear"),
            language=LanguageConfig(
                hidden_size=128, num_layers=1, vocab_size=256,
                mla=MLAConfig(q_lora_rank=64, kv_lora_rank=32, qk_rope_head_dim=8, qk_nope_head_dim=8, v_head_dim=16),
                moe=MoEConfig(enabled=False)
            ),
        )
        model = DeepSeekVL2ForPretraining(config).to(device)
        
        # Verify tying before save
        assert model.lm_head.weight is model.language_model.embed_tokens.weight
        
        with tempfile.TemporaryDirectory() as tmpdir:
            path = Path(tmpdir) / "model.pt"
            torch.save(model.state_dict(), path)
            
            # Create new model and load
            model2 = DeepSeekVL2ForPretraining(config).to(device)
            model2.load_state_dict(torch.load(path))
            
            # Check if tying is preserved
            # After load, weights should still be tied
            assert model2.lm_head.weight is model2.language_model.embed_tokens.weight
    
    def test_weight_tying_dim_mismatch(self, device):
        """
        BUG: If LM head and embedding have different shapes, tying silently fails.
        """
        config = ModelConfig(
            vision=VisionConfig(image_size=56, hidden_size=64, num_layers=1, num_heads=2, patch_size=8),
            projector=ProjectorConfig(input_dim=64, output_dim=128, type="linear"),
            language=LanguageConfig(
                hidden_size=128, num_layers=1, vocab_size=256,
                mla=MLAConfig(q_lora_rank=64, kv_lora_rank=32, qk_rope_head_dim=8, qk_nope_head_dim=8, v_head_dim=16),
                moe=MoEConfig(enabled=False)
            ),
        )
        model = DeepSeekVL2ForPretraining(config).to(device)
        
        # LM head and embedding should have same shape
        assert model.lm_head.weight.shape == model.language_model.embed_tokens.weight.shape
        assert model.lm_head.weight.shape == (256, 128)  # (vocab, hidden)


# =============================================================================
# BUG #6: Config Loading Edge Cases
# =============================================================================

class TestConfigBugs:
    """Test config loading bugs."""
    
    def test_nested_override_creates_missing_path(self):
        """
        BUG: If override path doesn't exist, getattr raises AttributeError.
        Line 292 in config.py: obj = getattr(obj, part)
        """
        config = DeepSeekVL2TrainingConfig()
        
        # This should raise or handle gracefully
        try:
            # Try to override a non-existent nested path
            overrides = {"model.nonexistent.value": 123}
            with tempfile.TemporaryDirectory() as tmpdir:
                path = Path(tmpdir) / "config.yaml"
                save_config(config, path)
                
                with pytest.raises(AttributeError):
                    load_config_with_overrides(path, overrides)
        except Exception as e:
            # Expected - should raise AttributeError
            pass
    
    def test_empty_config_yaml(self):
        """
        BUG: Empty YAML file should create valid default config.
        """
        with tempfile.TemporaryDirectory() as tmpdir:
            path = Path(tmpdir) / "config.yaml"
            with open(path, 'w') as f:
                f.write("")  # Empty file
            
            # yaml.safe_load returns None for empty file
            # This may crash load_config
            try:
                config = load_config(path)
                # If it doesn't crash, verify defaults
                assert config.model.variant == "tiny"
            except Exception:
                pytest.fail("Empty config file should load with defaults")
    
    def test_variant_not_in_variants_dict(self):
        """
        BUG: If variant name doesn't match any preset, no error is raised.
        """
        with tempfile.TemporaryDirectory() as tmpdir:
            path = Path(tmpdir) / "config.yaml"
            config_str = """
model:
  variant: nonexistent_variant
"""
            with open(path, 'w') as f:
                f.write(config_str)
            
            config = load_config(path)
            # Should load but with default values (variant preset not applied)
            assert config.model.variant == "nonexistent_variant"
            # Values should be defaults
            assert config.model.language.hidden_size == 2048  # default


# =============================================================================
# BUG #7: Language Model Gradient Checkpointing
# =============================================================================

class TestGradientCheckpointingBugs:
    """Test gradient checkpointing bugs."""
    
    def test_gradient_checkpointing_output_tuple(self, device):
        """
        BUG: Gradient checkpointing may fail if layer returns variable-length tuple.
        Line 146-155 in language_model.py passes return_router_logits, but
        checkpoint expects consistent outputs.
        """
        config = LanguageConfig(
            hidden_size=256, num_layers=2,
            mla=MLAConfig(q_lora_rank=128, kv_lora_rank=64, qk_rope_head_dim=16, qk_nope_head_dim=16, v_head_dim=32),
            moe=MoEConfig(enabled=True, num_experts=4, num_experts_per_token=2)
        )
        lm = DeepSeekMoELanguageModel(config).to(device)
        lm.gradient_checkpointing = True
        lm.train()
        
        input_ids = torch.randint(0, 1000, (2, 32), device=device)
        
        # With router logits
        outputs = lm(input_ids=input_ids, output_router_logits=True)
        assert outputs[0] is not None
    
    def test_gradient_checkpointing_with_cache_conflict(self, device):
        """
        BUG: Gradient checkpointing and use_cache may conflict.
        """
        config = LanguageConfig(
            hidden_size=256, num_layers=2,
            mla=MLAConfig(q_lora_rank=128, kv_lora_rank=64, qk_rope_head_dim=16, qk_nope_head_dim=16, v_head_dim=32),
            moe=MoEConfig(enabled=False)
        )
        lm = DeepSeekMoELanguageModel(config).to(device)
        lm.gradient_checkpointing = True
        lm.train()
        
        input_ids = torch.randint(0, 1000, (2, 32), device=device)
        
        # use_cache with gradient checkpointing often doesn't work
        # This may produce a warning or different behavior
        outputs = lm(input_ids=input_ids, use_cache=True)
        # Should at least not crash


# =============================================================================
# BUG #8: Loss Computation Edge Cases
# =============================================================================

class TestLossBugs:
    """Test loss computation bugs."""
    
    def test_loss_with_all_padding(self, device):
        """
        BUG: If all labels are -100, cross_entropy returns 0 or NaN.
        """
        config = ModelConfig(
            vision=VisionConfig(image_size=56, hidden_size=64, num_layers=1, num_heads=2, patch_size=8),
            projector=ProjectorConfig(input_dim=64, output_dim=128, type="linear"),
            language=LanguageConfig(
                hidden_size=128, num_layers=1, vocab_size=256,
                mla=MLAConfig(q_lora_rank=64, kv_lora_rank=32, qk_rope_head_dim=8, qk_nope_head_dim=8, v_head_dim=16),
                moe=MoEConfig(enabled=False)
            ),
        )
        model = DeepSeekVL2ForPretraining(config).to(device)
        
        input_ids = torch.randint(0, 256, (2, 16), device=device)
        labels = torch.full((2, 16), -100, device=device)  # All padding
        
        outputs = model(input_ids=input_ids, labels=labels)
        
        # Should be 0 or NaN (PyTorch returns nan when all ignored)
        assert outputs["loss"] is not None
    
    def test_loss_shift_with_short_sequence(self, device):
        """
        BUG: Causal shift (logits[:-1], labels[1:]) fails for seq_len=1.
        """
        config = ModelConfig(
            vision=VisionConfig(image_size=56, hidden_size=64, num_layers=1, num_heads=2, patch_size=8),
            projector=ProjectorConfig(input_dim=64, output_dim=128, type="linear"),
            language=LanguageConfig(
                hidden_size=128, num_layers=1, vocab_size=256,
                mla=MLAConfig(q_lora_rank=64, kv_lora_rank=32, qk_rope_head_dim=8, qk_nope_head_dim=8, v_head_dim=16),
                moe=MoEConfig(enabled=False)
            ),
        )
        model = DeepSeekVL2ForPretraining(config).to(device)
        
        # Sequence length 1
        input_ids = torch.randint(0, 256, (2, 1), device=device)
        labels = input_ids.clone()
        
        outputs = model(input_ids=input_ids, labels=labels)
        
        # After shift: logits[..., :-1, :] has 0 tokens
        # This creates empty tensors for loss
        # PyTorch handles this but loss may be NaN


# =============================================================================
# BUG #9: Vision Encoder Edge Cases
# =============================================================================

class TestVisionEncoderBugs:
    """Test vision encoder bugs."""
    
    def test_position_embedding_not_learnable(self):
        """
        BUG: If position embedding is not properly initialized or learnable.
        """
        config = VisionConfig(
            image_size=224, patch_size=16, hidden_size=256, num_layers=2, num_heads=4
        )
        encoder = SigLIPVisionEncoder(config)
        
        # Position embedding should be learnable parameter
        assert encoder.pos_embed.requires_grad == True
    
    def test_image_size_not_divisible_by_patch(self):
        """
        BUG: If image_size % patch_size != 0, patch embedding fails.
        """
        # This should raise an error during init or produce incorrect output
        config = VisionConfig(
            image_size=225,  # Not divisible by 16
            patch_size=16,
            hidden_size=256,
            num_layers=1,
            num_heads=4
        )
        encoder = SigLIPVisionEncoder(config)
        
        # Try to process - may fail or produce wrong shape
        x = torch.randn(1, 3, 225, 225)
        try:
            out = encoder(x)
            # Check if output is valid
            assert out.dim() == 3
        except Exception:
            # Expected for non-divisible sizes
            pass


# =============================================================================
# BUG #10: Initialization Issues
# =============================================================================

class TestInitBugs:
    """Test initialization bugs."""
    
    def test_weight_initialization_scale(self):
        """
        BUG: Weights may be initialized with wrong scale.
        """
        config = ModelConfig(
            vision=VisionConfig(image_size=56, hidden_size=64, num_layers=1, num_heads=2, patch_size=8),
            projector=ProjectorConfig(input_dim=64, output_dim=128, type="linear"),
            language=LanguageConfig(
                hidden_size=128, num_layers=1, vocab_size=256,
                mla=MLAConfig(q_lora_rank=64, kv_lora_rank=32, qk_rope_head_dim=8, qk_nope_head_dim=8, v_head_dim=16),
                moe=MoEConfig(enabled=False)
            ),
        )
        model = DeepSeekVL2ForPretraining(config)
        
        # Check weights are not too large or too small
        for name, param in model.named_parameters():
            if 'weight' in name and param.dim() > 1:
                std = param.std().item()
                assert 1e-6 < std < 10.0, f"Weight {name} has bad std: {std}"
    
    def test_special_embeddings_initialization(self):
        """
        BUG: image_newline and view_separator might have wrong scale.
        Line 54-59 in deepseek_vl2.py initializes with std = 1/sqrt(dim).
        """
        config = ModelConfig(
            vision=VisionConfig(image_size=56, hidden_size=64, num_layers=1, num_heads=2, patch_size=8),
            projector=ProjectorConfig(input_dim=64, output_dim=128),
            language=LanguageConfig(
                hidden_size=128, num_layers=1, vocab_size=256,
                mla=MLAConfig(q_lora_rank=64, kv_lora_rank=32, qk_rope_head_dim=8, qk_nope_head_dim=8, v_head_dim=16),
                moe=MoEConfig(enabled=False)
            ),
        )
        model = DeepSeekVL2ForPretraining(config)
        
        # Expected std = 1/sqrt(128) â‰ˆ 0.088
        expected_std = 1 / (128 ** 0.5)
        actual_std = model.image_newline.std().item()
        
        # Check it's in reasonable range
        assert abs(actual_std - expected_std) < expected_std * 2


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
